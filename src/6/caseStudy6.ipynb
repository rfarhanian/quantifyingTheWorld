{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "Nolan and Lang try to predict the location of devices using different access points in a building. They use K nearest neighbor algorithm. In this case study, we initially investigate the Nolan and Lang code and analyze the impact of an extra access point that they dropped from the dataset. We will ask ourselves if it was a good decision. We will also analyze if retaining the access point will lead to a more accurate result. Finally, we will use a weighted Euclidean distance technique to predict the location. Initially we analyze the Nolan and Lang code line by line and then we will try to answer the mentioned questions. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the data\n",
    "Following code will read the data from the url and store it in txt variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "\n",
    "options(digits = 2)\n",
    "\n",
    "txt <- readLines('data/offline.final.trace.txt')\n",
    "\n",
    "head(txt)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "Following lines explains the data:\n",
    "\n",
    "* t  \"Timestamp\" \n",
    "* id \"MACofScanDevice\"\n",
    "* pos \"RealPosition\"\n",
    "* degree \"orientation\"\n",
    "* MACofResponse1 = \"SignalStrengthValue,Frequency,Mode\" ...\n",
    "* MACofResponseN = \"SignalStrengthValue,Frequency,Mode\"\n",
    "\n",
    "These reference locations give us a calibration set of signal strengths for the building, and we use them to build our model to predict the locations of the hand-held device when its position is unknown.\n",
    "\n",
    "The columns are self explanatory but some of the above items are compound and should be transformed and cleansed. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to clean the data to transform it into a dataframe. This can be done in two ways:\n",
    "\n",
    "\n",
    "## First Approach\n",
    "\n",
    "The first format is to transform each line to reach the following format:\n",
    "\n",
    "t(timestamp), id(MAC), pos(Real Position), degree, Mac1(first observation of the device), Mac2(second observation), ..., Mac N\n",
    "\n",
    "### Advantage\n",
    "Each line in the file corresponds to one line in the dataframe.\n",
    "\n",
    "\n",
    "### Disadvantage\n",
    "The number of columns in each line vary as there might be different observations of the data. Hence we need two passes of the data to create the dataframe.\n",
    "\n",
    "\n",
    "## Second Approach\n",
    "\n",
    "A second approach is to have the same initial variables describing the hand-held device, i.e., time, MAC address, location, and orientation. After this, we have just 4 other variables: the MAC address of the device from which we received a signal, the signal, the channel and the type of the device.\n",
    "\n",
    "t(timestamp), id(MAC), pos(Real Position), degree, sender signal, signal, channel, device type\n",
    "\n",
    "\n",
    "### Advantage\n",
    "* The data can be read in one shot.\n",
    "* We can use group by. \n",
    "\n",
    "\n",
    "### Disadvantage\n",
    "The number of columns in each line vary as there might be different observations of the data. Hence we need two passes of the data to create the dataframe.\n",
    "\n",
    "## Data Cleansing\n",
    "We will follow the second approach. Commented lines that start with # should also be removed. We will split the 4th column which is a composite column based on semicolon, equal and comma. Then we make sure to choose the rows with valid columns and then focus on the types of numeric columns.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum(substr(txt, 1, 1) == \"#\")\n",
    "\n",
    "length(txt)\n",
    "\n",
    " strsplit(txt[4], \";\")[[1]]\n",
    "\n",
    " tokens = strsplit(txt[4], \"[;=,]\")[[1]]\n",
    "\n",
    "tokens[1:10]\n",
    "\n",
    " tokens[c(2, 4, 6:8, 10)]\n",
    "\n",
    " tokens[  -( 1:10 ) ]\n",
    "\n",
    " tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE)\n",
    " \n",
    " mat = cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE), tmp)\n",
    "\n",
    "  dim(mat)\n",
    "\n",
    "processLine = function(x)\n",
    "{\n",
    "  tokens = strsplit(x, \"[;=,]\")[[1]]\n",
    "  tmp = matrix(tokens[ - (1:10) ], ncol = 4, byrow = TRUE)\n",
    "  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp),\n",
    "               ncol = 6, byrow = TRUE), tmp)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tmp = lapply(txt[4:20], processLine)\n",
    "\n",
    " sapply(tmp, nrow)\n",
    "\n",
    " offline = as.data.frame(do.call(\"rbind\", tmp))\n",
    " \n",
    " dim(offline)\n",
    "\n",
    " lines = txt[ substr(txt, 1, 1) != \"#\" ]\n",
    " tmp = lapply(lines, processLine)\n",
    "\n",
    "processLine = function(x)\n",
    "{\n",
    "  tokens = strsplit(x, \"[;=,]\")[[1]]\n",
    "  \n",
    "  if (length(tokens) == 10) \n",
    "    return(NULL)\n",
    " \n",
    "  tmp = matrix(tokens[ - (1:10) ], , 4, byrow = TRUE)\n",
    "  cbind(matrix(tokens[c(2, 4, 6:8, 10)], nrow(tmp), 6, \n",
    "               byrow = TRUE), tmp)\n",
    "}\n",
    "\n",
    " options(error = recover, warn = 1)\n",
    " tmp = lapply(lines, processLine)\n",
    " offline = as.data.frame(do.call(\"rbind\", tmp), stringsAsFactors = FALSE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Type Check\n",
    "Now the data is ready to be used as a dataframe. The dimension of the data indicates that all 1181628 rows have 10 columns and we can check the types of the variables in the data frame and verify that they are correct. The \"rawTime\" will also be converted into milliseconds as \"time\". "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim(offline)\n",
    "# head(offline)\n",
    "names(offline) = c(\"time\", \"scanMac\", \"posX\", \"posY\", \"posZ\", \n",
    "                   \"orientation\", \"mac\", \"signal\", \n",
    "                   \"channel\", \"type\")\n",
    "\n",
    "numVars = c(\"time\", \"posX\", \"posY\", \"posZ\", \n",
    "            \"orientation\", \"signal\")\n",
    "offline[ numVars ] =  lapply(offline[ numVars ], as.numeric)\n",
    "\n",
    "offline = offline[ offline$type == \"3\", ]\n",
    "offline = offline[ , \"type\" != names(offline) ]\n",
    "#offline[1:2]\n",
    "dim(offline)\n",
    "\n",
    "offline$rawTime = offline$time\n",
    "offline$time = offline$time/1000\n",
    "class(offline$time) = c(\"POSIXt\", \"POSIXct\")\n",
    "\n",
    "unlist(lapply(offline, class))  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(offline[, numVars])\n",
    "summary(sapply(offline[ , c(\"mac\", \"channel\", \"scanMac\")], as.factor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## posX and scanMac\n",
    "The following code prints some information about the numeric and character type columns.\n",
    "It seems that posZ is always zero (We might remove this variable later) and the scanMac value is always same which indicates that all of the measurements were taken on one floor of the building. We can eliminate this variable as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline = offline[ , !(names(offline) %in% c(\"scanMac\", \"posZ\"))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Orientation Analysis\n",
    " The following code indicate that we have 203 different orientation values which is not helpful. In order to predict the location of the device, we should predict movement. If we consolidate the existing values to only 8 values for orientation (e.g. 0, 45, 90, ..., 315), it will help us predict the location more efficiently.  \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length(unique(offline$orientation))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 and 360 orientation overlap problem\n",
    "This empirical distribution function of orientation shows that there are 8 basic orientations that are 45 degrees apart. We observe from the steps in the function that these orientations are not exactly 45, 90, 135, etc. Also, the 0 orientation is split into the two groups, one near 0 and the other near 360 as they both refer to the same orientation. Nolan and Lang use roundOrientation function to address this issue and introduce a new variable called angle.\n",
    " \n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(ecdf(offline$orientation), main=\"8 equi-spaced angles\", sub=\"Emperical cumulative distribution function transforms orientations into 8 segments.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_ECDFOrientation.pdf\", width = 10, height = 7)\n",
    "oldPar = par(mar = c(4, 4, 1, 1))\n",
    "plot(ecdf(offline$orientation), pch = 19, cex = 0.3,\n",
    "     xlim = c(-5, 365), axes = FALSE,\n",
    "     xlab = \"orientation\", ylab = \"Empirical CDF\", main = \"\")\n",
    "box()\n",
    "axis(2)\n",
    "axis(side = 1, at = seq(0, 360, by = 45))\n",
    "par(oldPar)\n",
    "dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_DensityOrientation.pdf\", width = 10, height = 5)\n",
    "oldPar = par(mar = c(4, 4, 1, 1))\n",
    "plot(density(offline$orientation, bw = 2), \n",
    " xlab = \"orientation\", main = \"\")\n",
    "title(\"Density vs. Orientation\")\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "roundOrientation = function(angles) {\n",
    "  refs = seq(0, by = 45, length  = 9)\n",
    "  q = sapply(angles, function(o) which.min(abs(o - refs)))\n",
    "  c(refs[1:8], 0)[q]\n",
    "}\n",
    "\n",
    "offline$angle = roundOrientation(offline$orientation)\n",
    "\n",
    "#pdf(file = \"Geo_BoxplotAngle.pdf\", width = 10)\n",
    "oldPar = par(mar = c(4, 4, 1, 1))\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the summary() information, it seems that there may be a one-to-one mapping between the MAC address of the access \n",
    "points and channel. For example, the summary statistics show there are 126,529 occurrences of the address 00:14:bf:3b:c7:c6 and the same number of occurrences of channel 2432000000. To help us ascertain if we do have a one- to-one mapping, we look at the relationship between the MAC address and channel.\n",
    "How many unique addresses and channels do we have? There should be the same number, if there is a one-to-one mapping. We find the following:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c(length(unique(offline$mac)), length(unique(offline$channel)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 12 MAC addresses and 8 channels. We were given the impression from the building plan that there are only 6 access points. Why are there 8 channels and 12 MAC addresses? Rereading the documentation we find that there are additional access points that are not part of the testing area and so not seen on the floor plan. Let’s check the counts of observations for the various MAC addresses with table():"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Access Points\n",
    "According to the documentation, the access points consist of 5 Linksys/Cisco and one Lancom L-54g routers. We look up these MAC addresses at the http://coffer.com/mac_find/ site to find the vendor addresses that begin with 00:14:bf belong to Linksys devices, those beginning with 00:0f:a3 belong to Alpha Networks, and Lancom devices start with 00:a0:57 (see Figure 1.4). We do have 5 devices with an address that begins 00:14:bf, which matches with the Linksys count from the documentation. However, none of our MAC addresses begin with 00:a0:57 so there is a discrepancy with the documentation. Please recall that there are potentially signals recorded at 166 grid points, 8 orientations, and 110 replications. \n",
    "\n",
    "The third and fifth above addresses are not among the access points displayed on the map because they have much lower counts than the others and these are far lower than the possible 146,080 recordings. For now, we keep records of our top 7 devices.   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "table(offline$mac)\n",
    "\n",
    "subMacs = names(sort(table(offline$mac), decreasing = TRUE))[1:7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to one to one correspondence of channel and MAC address of our top 7 devices, we eliminate the channel. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline = offline[ offline$mac %in% subMacs, ]\n",
    "macChannel = with(offline, table(mac, channel))\n",
    "apply(macChannel, 1, function(x) sum(x > 0))\n",
    "\n",
    "offline = offline[ , \"channel\" != names(offline)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Position of the Hand-Held Device\n",
    "we consider the position variables, posX and posY. For how many different locations do we have data? The by() function can tally up the numbers of rows in our data frame for each unique (x, y) combination. We begin by creating a list containing a data frame for each location as follows and remove the 310 null elements:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locDF = with(offline, \n",
    "             by(offline, list(posX, posY), function(x) x))\n",
    "length(locDF)\n",
    "\n",
    "\n",
    "sum(sapply(locDF, is.null))\n",
    "\n",
    "locDF = locDF[ !sapply(locDF, is.null) ]\n",
    "\n",
    "length(locDF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now operate on all 166 data frames to determine the number of observations recorded\n",
    "at each location. The next sapply function will keep the position information with the location and then we can confirm that \"locCounts\" is a matrix with 3 rows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "locCounts = sapply(locDF, nrow)\n",
    "\n",
    "locCounts = sapply(locDF, \n",
    "                   function(df) \n",
    "                     c(df[1, c(\"posX\", \"posY\")], count = nrow(df)))\n",
    "\n",
    "class(locCounts)\n",
    "\n",
    "dim(locCounts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the following code illustrates, examining a few of the location data indicates that we have about 5500 records per position. With 7 access points in place and the 8 orientations that we cleansed, there are 110 replications of the data. This becomes 6,160 signal strength measurements. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This matrix can be plotted and it gives us a very good overview of the presence of devices on different locations. The following code illustrates this data and as you can see, not all signals were detected, so there are about 5500 recordings at each location:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(offline$time, offline$signal, xlab=\"Time\", ylab=\"Signal\", \n",
    "     main=\"Signal vs. Time Plot\", sub=\"Signal is less dense and less variant in March.\")\n",
    "plot(offline$time, offline$orientation, xlab=\"Time\", ylab=\"Orientation\", \n",
    "     main=\"Orientation vs. Time Plot\", , sub=\"Orientation is less dense and less variant in March.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(offline$time, offline$channel, ylab=\"Time\", xlab=\"Channel\", \n",
    "     main=\"Channel vs. Time Plot\", sub=\"Channel is less variant in March.\")\n",
    "plot(offline$time, offline$scanMac, xlab=\"Time\", ylab=\"Signal\",\n",
    "     main=\"ScanMac vs. Time Plot\", sub=\"ScanMac is less variant in March.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(offline$time, offline$posX, ylab=\"posX\", xlab=\"Time\",  \n",
    "     main=\"posX vs. Time Plot\",\n",
    "     sub=\"posX seems to be very limited in variance and volume in March.\")\n",
    "plot(offline$time, offline$posY, ylab=\"posY\", xlab=\"Time\",\n",
    "     main=\"posY vs. Time Plot\",\n",
    "     sub=\"posY seems to be very limited in variance and volume in March.\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(offline$time, offline$posZ, ylab=\"posZ\", xlab=\"Time\",\n",
    "     main=\"posZ vs. Time Plot\", \n",
    "     sub=\"posZ seems to be very limited in variance and volume in March.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots of different variables against time\n",
    "\n",
    "Nolan and Lang left this exercise for us to detect any time related bias in the data as they examined all variables except \"time\" and \"signal\". By plotting variable against the time, we observe that data has been captured in three dates in February and March. The plots above indicates that we have less data in March compared to February which might cause a bias. We will consider this as part of our analysis.    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_XYByCount.pdf\", width = 10)\n",
    "oldPar = par(mar = c(3.1, 3.1, 1, 1))\n",
    "\n",
    "locCounts = t(locCounts)\n",
    "plot(locCounts, type = \"n\", xlab = \"\", ylab = \"\")\n",
    "text(locCounts, labels = locCounts[,3], cex = .8, srt = 45)\n",
    "\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to cleanse the signal data now. As explained in Section 1.3.4 of Data Science in R, the following process introduces a function to redo all cleansing we have done in \"readData\" function. It reuses the roundRotation and processLine functionality that we have globally shared in the R session. Then we use the idential functionality to verify that the output of \"readData\" function is identical to the existing \"offline\" variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "readData = \n",
    "  function(filename = 'data/offline.final.trace.txt', \n",
    "           subMacs = c(\"00:0f:a3:39:e1:c0\", \"00:0f:a3:39:dd:cd\", \"00:14:bf:b1:97:8a\",\n",
    "                       \"00:14:bf:3b:c7:c6\", \"00:14:bf:b1:97:90\", \"00:14:bf:b1:97:8d\",\n",
    "                       \"00:14:bf:b1:97:81\"))\n",
    "  {\n",
    "    txt = readLines(filename)\n",
    "    lines = txt[ substr(txt, 1, 1) != \"#\" ]\n",
    "    tmp = lapply(lines, processLine)\n",
    "    offline = as.data.frame(do.call(\"rbind\", tmp), \n",
    "                            stringsAsFactors= FALSE) \n",
    "    \n",
    "    names(offline) = c(\"time\", \"scanMac\", \n",
    "                       \"posX\", \"posY\", \"posZ\", \"orientation\", \n",
    "                       \"mac\", \"signal\", \"channel\", \"type\")\n",
    "    \n",
    "     # keep only signals from access points\n",
    "    offline = offline[ offline$type == \"3\", ]\n",
    "    \n",
    "    # drop scanMac, posZ, channel, and type - no info in them\n",
    "    dropVars = c(\"scanMac\", \"posZ\", \"channel\", \"type\")\n",
    "    offline = offline[ , !( names(offline) %in% dropVars ) ]\n",
    "    \n",
    "    # drop more unwanted access points\n",
    "    offline = offline[ offline$mac %in% subMacs, ]\n",
    "    offline\n",
    "    # convert numeric values\n",
    "    numVars = c(\"time\", \"posX\", \"posY\", \"orientation\", \"signal\")\n",
    "    offline[ numVars ] = lapply(offline[ numVars ], as.numeric)\n",
    "\n",
    "    # convert time to POSIX\n",
    "    offline$rawTime = offline$time\n",
    "    offline$time = offline$time/1000\n",
    "    class(offline$time) = c(\"POSIXt\", \"POSIXct\")\n",
    "    \n",
    "    # round orientations to nearest 45\n",
    "    offline$angle = roundOrientation(offline$orientation)\n",
    "      \n",
    "    return(offline)\n",
    "  }\n",
    "\n",
    "offlineRedo = readData()\n",
    "\n",
    "identical(offline, offlineRedo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "  We have measured the signal strength to an access point multiple times at each location and orientation. How do these signal strengths behave? That is, what is the distribution of the repeated measurements at each location and orientation? Does signal strength behave similarly at all locations? Or does, the location, orientation, and access point affect this distribution?\n",
    "\n",
    " In practice, physical characteristics of a building and human activity can add significant noise to signal strength measurements. How can we characterize the relationship between the signal strength and the distance from the device to the access point? How does the orientation affect this relationship? Is this relationship the same for all access points?\n",
    "\n",
    "We wil try to find answer for these questions.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distribution of Signal Strength\n",
    "We want to compare the distribution of signal strength at different orientations and for different access points, so we need to subdivide our data. We are interested in seeing if these distributions are normal or skewed. We also want to look at their variances.\n",
    "\n",
    "We would like to consider the impact of orientation on signal strength by fixing a location on the map to see how the signal changes as the experimenter rotates through the 8 angles. We also separately examine the MAC addresses because, for example, at an orientation of 90 degrees the experimenter may be facing toward one access point and away from another. As part of the work, we summarize the signal values. The more negative the signal value is, the weaker it becomes. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_BoxplotSignalByMacAngle.pdf\", width = 7)\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "library(lattice)\n",
    "bwplot(signal ~ factor(angle) | mac, data = offline, \n",
    "       subset = posX == 2 & posY == 12 \n",
    "                & mac != \"00:0f:a3:39:dd:cd\", \n",
    "       layout = c(2,3))\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "summary(offline$signal)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mac address and angle correlation\n",
    "We observe in the previous plot that the signal strength varies with the orientation for both close and distant access points. Note we have dropped the records for the MAC address of 00:0f:a3:39:dd:cd because it is identified as the extra address in the next section.\n",
    "\n",
    "In the following code, we compare the distributions of signal strength for different angles and MAC addresses at the central location of x = 23 and y = 4. Lack of normal distribution in the following plot illustrates that conditioning on angle and MAC address is warranted. If the distributions were normal, we could conclude that signal can be received from different angles but that does not seem to be the case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_DensitySignalByMacAngle.pdf\", width = 8, height = 12)\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "densityplot( ~ signal | mac + factor(angle), data = offline,\n",
    "             subset = posX == 24 & posY == 4 & \n",
    "                         mac != \"00:0f:a3:39:dd:cd\",\n",
    "             bw = 0.5, plot.points = FALSE)\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want to examine the distribution of signal strength for all 166 locations, 8 angles, and 6 access points, we need to create thousands of boxplots or density curves. We can, instead, examine summary statistics such as the mean and SD or the median and IQR of signal strength for all location–orientation–access point combinations. For each combination, we have roughly 100 observations. To compute summary statistics for these various combinations, we first create a special factor that contains all of the unique combinations of the observed (x, y) pairs for the 166 locations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline$posXY = paste(offline$posX, offline$posY, sep = \"-\")\n",
    "\n",
    "byLocAngleAP = with(offline, \n",
    "                    by(offline, list(posXY, angle, mac), \n",
    "                       function(x) x))\n",
    "\n",
    "signalSummary = \n",
    "  lapply(byLocAngleAP,            \n",
    "         function(oneLoc) {\n",
    "           ans = oneLoc[1, ]\n",
    "           ans$medSignal = median(oneLoc$signal)\n",
    "           ans$avgSignal = mean(oneLoc$signal)\n",
    "           ans$num = length(oneLoc$signal)\n",
    "           ans$sdSignal = sd(oneLoc$signal)\n",
    "           ans$iqrSignal = IQR(oneLoc$signal)\n",
    "           ans\n",
    "           })\n",
    "\n",
    "offlineSummary = do.call(\"rbind\", signalSummary)                             "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the offlineSummary data and observe that the weakest signals have small standard deviations and that it appears that the SD increases with the average signal strength. If we plan to model the behavior of signal strength, then we want to take these features into consideration. The weak signals have low variability and the stronger signals have greater variability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_BoxplotSignalSDByAvg.pdf\", width = 10)\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "breaks = seq(-90, -30, by = 5)\n",
    "bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),\n",
    "       data = offlineSummary, \n",
    "       subset = mac != \"00:0f:a3:39:dd:cd\",\n",
    "       xlab = \"Mean Signal\", ylab = \"SD Signal\")\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We examine the skewness of signal strength by plotting the difference, avgSignal - medSignal, against the number of observations. Then we use the fitted model to predict the difference for each value of num and add these predictions to the scatter plot. We see that these two measures of centrality are similar to each other; they typically differ by less than 1 to 2 dBm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_ScatterMean-Median.pdf\", width = 10)\n",
    "oldPar = par(mar = c(4.1, 4.1, 1, 1))\n",
    "\n",
    "with(offlineSummary,\n",
    "     smoothScatter((avgSignal - medSignal) ~ num,\n",
    "                   xlab = \"Number of Observations\", \n",
    "                   ylab = \"mean - median\"))\n",
    "abline(h = 0, col = \"#984ea3\", lwd = 2)\n",
    "\n",
    "lo.obj = \n",
    "  with(offlineSummary,\n",
    "       loess(diff ~ num, \n",
    "             data = data.frame(diff = (avgSignal - medSignal),\n",
    "                               num = num)))\n",
    "\n",
    "lo.obj.pr = predict(lo.obj, newdata = data.frame(num = (70:120)))\n",
    "lines(x = 70:120, y = lo.obj.pr, col = \"#4daf4a\", lwd = 2)\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    " \n",
    "oneAPAngle = subset(offlineSummary, \n",
    "                    mac == subMacs[5] & angle == 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list.of.packages <- c(\"fields\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages)\n",
    "\n",
    "\n",
    "library(fields)\n",
    "smoothSS = Tps(oneAPAngle[, c(\"posX\",\"posY\")], \n",
    "               oneAPAngle$avgSignal)\n",
    "\n",
    "vizSmooth = predictSurface(smoothSS)\n",
    "\n",
    "plot.surface(vizSmooth, type = \"C\")\n",
    "\n",
    "points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we see that we can easily identify the location of the access point as the dark red region at the top of the “mountain.” We also confirm the effect of the orientation on signal strength. Additionally, a corridor effect emerges. \n",
    "\n",
    "The signal is stronger relative to distance along the corridors where the signals are not blocked by walls.\n",
    "We know the locations of the access points based on the floor plan of the building, but we have not been given their exact location and we do not know the mapping between MAC address and access point. Fortunately, the contour maps that we just created make it easy to connect the MAC address to the access point marked on the floor plan in figure 1.1.\n",
    "\n",
    "For example, the signals appearing in the top row of the plot clearly correspond to the access point in the top left corner of the building. Also, according to the documentation, the training data were measured at 1 meter intervals in the building so we can use the grey dots on the plan to estimate the location of the access points. We find that two MAC addresses have similar heat maps and these both correspond to the access point near the center of the building (i.e., x = 7.5 and y = 6.3). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique(offlineSummary$mac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "surfaceSS = function(data, mac, angle = 45) {\n",
    "  require(fields)\n",
    "  oneAPAngle = data[ data$mac == mac & data$angle == angle, ]\n",
    "  smoothSS = Tps(oneAPAngle[, c(\"posX\",\"posY\")], \n",
    "                 oneAPAngle$avgSignal)\n",
    "  vizSmooth = predictSurface(smoothSS)\n",
    "  plot.surface(vizSmooth, type = \"C\", \n",
    "               xlab = \"\", ylab = \"\", xaxt = \"n\", yaxt = \"n\")\n",
    "  points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5) \n",
    "}\n",
    "\n",
    " parCur = par(mfrow = c(2,2), mar = rep(1, 4))\n",
    "\n",
    " mapply(surfaceSS, mac = subMacs[ rep(c(5, 1), each = 2) ], \n",
    "        angle = rep(c(0, 135), 2),\n",
    "        data = list(data = offlineSummary))\n",
    " \n",
    " par(parCur)\n",
    "\n",
    " offlineSummary = subset(offlineSummary, mac != subMacs[2])\n",
    "\n",
    "AP = matrix( c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,  \n",
    "                1, 14, 33.5, 9.3,  33.5, 2.8),\n",
    "            ncol = 2, byrow = TRUE,\n",
    "            dimnames = list(subMacs[ -2 ], c(\"x\", \"y\") ))\n",
    "\n",
    "   AP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "diffs = offlineSummary[ , c(\"posX\", \"posY\")] - \n",
    "          AP[ offlineSummary$mac, ]\n",
    "\n",
    "offlineSummary$dist = sqrt(diffs[ , 1]^2 + diffs[ , 2]^2)\n",
    "\n",
    "xyplot(signal ~ dist | factor(mac) + factor(angle), \n",
    "       data = offlineSummary, pch = 19, cex = 0.3,\n",
    "       xlab =\"distance\")\n",
    "\n",
    "#pdf(file=\"Geo_ScatterSignalDist.pdf\", width = 7, height = 10)\n",
    "oldPar = par(mar = c(3.1, 3.1, 1, 1))\n",
    "library(lattice)\n",
    "xyplot(signal ~ dist | factor(mac) + factor(angle), \n",
    "       data = offlineSummary, pch = 19, cex = 0.3,\n",
    "       xlab =\"distance\")\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "macs = unique(offlineSummary$mac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " At this point, we have a set of training data that we can use to predict the location of our new point. We want to look at the distance in terms of signal strengths from these training data to the new point. If we would like to use the nearest neighbor or the 3 nearest neighbors, we need to calculate the distance from the new point to all observations in the training set.\n",
    " The following code prepares the test data by talling the number of signal strengths recorded at each location, keeping the data variables that is being used in assessing prediction. This new data frame will have 60 rows and 11 variables, including 6 average signal strengths at the corresponding MAC addresses. The dimension of onlineSummary will verify the result. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "online = readData(\"data/online.final.trace.txt\", subMacs = macs)\n",
    "\n",
    "online$posXY = paste(online$posX, online$posY, sep = \"-\")\n",
    "\n",
    "length(unique(online$posXY))\n",
    "\n",
    "tabonlineXYA = table(online$posXY, online$angle)\n",
    "tabonlineXYA[1:6, ]\n",
    "\n",
    "keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "byLoc = with(online, \n",
    "             by(online, list(posXY), \n",
    "                function(x) {\n",
    "                  ans = x[1, keepVars]\n",
    "                  avgSS = tapply(x$signal, x$mac, mean)\n",
    "                  y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                        dimnames = list(ans$posXY, names(avgSS)))\n",
    "                  cbind(ans, y)\n",
    "                }))\n",
    "\n",
    "onlineSummary = do.call(\"rbind\", byLoc) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the nearest neighbor model, we would like to find records in our offline data that have similiar orientations to our new observation as orientation can impact the strength of the signal. That is why we consider using all records with an orientation that is within a specified range of the new point's orientation. \n",
    "\n",
    "Since the observations were recorded in 45 degree increments, we can simply specify the number of neighboring angles to include from the training data. For example, if we want only one orientation, then we only include training data with angles that match the rounded orientation value of the new observation. If we want two orientations then we pick those two multiples of 45 degrees that flank the new observation’s orientation; for three, we choose the closest 45 degree increment and one on either side of it, and so on.\n",
    "\n",
    "The following code that will later be used in selectTrain function will help us enable the choice of orientation. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim(onlineSummary)\n",
    "\n",
    "names(onlineSummary)\n",
    "m = 3; angleNewObs = 230\n",
    "refs = seq(0, by = 45, length  = 8)\n",
    "nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "if (m %% 2 == 1) {\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "} else {\n",
    "  m = m + 1\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  if (sign(angleNewObs - nearestAngle) > -1) \n",
    "    angles = angles[ -1 ]\n",
    "  else \n",
    "    angles = angles[ -m ]\n",
    "}\n",
    "angles = angles + nearestAngle\n",
    "angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "angles[angles > 360] = angles[ angles > 360 ] - 360"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code introduces a reshapeSS function that aggregates the signal strengths for the angles. Then we will summarize and reshape \"offlineSubset\" data in trainSS variable.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offlineSubset = \n",
    "  offlineSummary[ offlineSummary$angle %in% angles, ]\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\")) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}\n",
    "\n",
    "trainSS = reshapeSS(offlineSubset, varSignal = \"avgSignal\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code reuses the above functionality to enable the choice of orientation and aggregate the signal strengths for the angles. It averages the signal strength for different angles to produce one set of signal stengths for each of the 166 locations in the training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selectTrain = function(angleNewObs, signals = NULL, m = 1){\n",
    "  # m is the number of angles to keep between 1 and 5\n",
    "  refs = seq(0, by = 45, length  = 8)\n",
    "  nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "  if (m %% 2 == 1) \n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  else {\n",
    "    m = m + 1\n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "    if (sign(angleNewObs - nearestAngle) > -1) \n",
    "      angles = angles[ -1 ]\n",
    "    else \n",
    "      angles = angles[ -m ]\n",
    "  }\n",
    "  angles = angles + nearestAngle\n",
    "  angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "  angles[angles > 360] = angles[ angles > 360 ] - 360\n",
    "  angles = sort(angles) \n",
    "  \n",
    "  offlineSubset = signals[ signals$angle %in% angles, ]\n",
    "  reshapeSS(offlineSubset, varSignal = \"avgSignal\")\n",
    "}\n",
    "\n",
    "train130 = selectTrain(130, offlineSummary, m = 3)\n",
    "\n",
    "head(train130)\n",
    "\n",
    "length(train130[[1]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a set of training data that we can use to predict the location of our new point. We want to look at the distance in terms of signal stength from these training data to the new point. So we need to calculate the distance from the new point to all observations in the training set. The findNN function will help us to do the calculation. It will return the locations of the training observations in order of closeness to the new observation's singal strength.  Later, you will see that a subset of this result can be used to estimate the location of the new observation(For k value of nearest neighbors, we can simply average the first k locations)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "findNN = function(newSignal, trainSubset) {\n",
    "  diffs = apply(trainSubset[ , 4:9], 1, \n",
    "                function(x) x - newSignal)\n",
    "  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )\n",
    "  closest = order(dists)\n",
    "  return(trainSubset[closest, 1:3])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function can predict the location for all dataset by reusing trainSelect and findNN functions.\n",
    "Please consider that we calculate the average of k neighbor distance using anonymous function using Euclidean distance. This function can later be imporved to be passed as a parameter to calculate the predicted location.\n",
    "\n",
    "The \"predXY\" is then invoked using 1 and 3 nearest neighbors and 3 orientations. We will later calculate the accuracy of our predictions with different number of neighbors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predXY = function(newSignals, newAngles, trainData, numAngles = 1, k = 3) {\n",
    "  \n",
    "  closeXY = list(length = nrow(newSignals))\n",
    "  \n",
    "  for (i in 1:nrow(newSignals)) {\n",
    "    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)\n",
    "    closeXY[[i]] = \n",
    "      findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)\n",
    "  }\n",
    "\n",
    "  estXY = lapply(closeXY, \n",
    "                 function(x) sapply(x[ , 2:3], \n",
    "                                    function(x) mean(x[1:k])))\n",
    "  estXY = do.call(\"rbind\", estXY)\n",
    "  return(estXY)\n",
    "}\n",
    "estXYk3 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 3)\n",
    "                                    \n",
    "\n",
    "estXYk1 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the errors\n",
    "The following function plots the errors for the predicated locations using different nearest neighbor numbers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "floorErrorMap = function(estXY, actualXY, trainPoints = NULL, AP = NULL) {\n",
    "  \n",
    "    plot(0, 0, xlim = c(0, 35), ylim = c(-3, 15), type = \"n\",\n",
    "         xlab = \"\", ylab = \"\", axes = FALSE)\n",
    "    box()\n",
    "    if ( !is.null(AP) ) points(AP, pch = 15)\n",
    "    if ( !is.null(trainPoints) )\n",
    "      points(trainPoints, pch = 19, col=\"grey\", cex = 0.6)\n",
    "    \n",
    "    points(x = actualXY[, 1], y = actualXY[, 2], \n",
    "           pch = 19, cex = 0.8 )\n",
    "    points(x = estXY[, 1], y = estXY[, 2], \n",
    "           pch = 8, cex = 0.8 )\n",
    "    segments(x0 = estXY[, 1], y0 = estXY[, 2],\n",
    "             x1 = actualXY[, 1], y1 = actualXY[ , 2],\n",
    "             lwd = 2, col = \"red\")\n",
    "}\n",
    "\n",
    "trainPoints = offlineSummary[ offlineSummary$angle == 0 & \n",
    "                              offlineSummary$mac == \"00:0f:a3:39:e1:c0\" ,\n",
    "                        c(\"posX\", \"posY\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file=\"GEO_FloorPlanK3Errors.pdf\", width = 10, height = 7)\n",
    "oldPar = par(mar = c(1, 1, 1, 1))\n",
    "floorErrorMap(estXYk3, onlineSummary[ , c(\"posX\",\"posY\")], \n",
    "              trainPoints = trainPoints, AP = AP)\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "#pdf(file=\"GEO_FloorPlanK1Errors.pdf\", width = 10, height = 7)\n",
    "oldPar = par(mar = c(1, 1, 1, 1))\n",
    "floorErrorMap(estXYk1, onlineSummary[ , c(\"posX\",\"posY\")], \n",
    "              trainPoints = trainPoints, AP = AP)\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Root Mean Square Error\n",
    "While the above code visualizes the error, the calcError function calculates the root mean square error. The following result 411.6403 is larger number compared to 270.458. This indicates that the root mean square error of the predicted result with 3 nearest neighbor is considerably more accurate. But we might still do better with different number of nearest neighbors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "calcError = \n",
    "function(estXY, actualXY) \n",
    "   sum( rowSums( (estXY - actualXY)^2) )\n",
    "\n",
    "actualXY = onlineSummary[ , c(\"posX\", \"posY\")]\n",
    "sapply(list(estXYk1, estXYk3), calcError, actualXY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we create v fold of subset of offline data as test data and use a subset of corresponding online data, we can find the K-NN estimate for a large K number for a certain number of angles with average signal strength. This will help us avoid overfitting. The following code help us to create the essential subset data with v =11 to create roughly equal folds across. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = 11\n",
    "permuteLocs = sample(unique(offlineSummary$posXY))\n",
    "permuteLocs = matrix(permuteLocs, ncol = v, \n",
    "                     nrow = floor(length(permuteLocs)/v))\n",
    "\n",
    "onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\"),\n",
    "                     sampleAngle = FALSE, \n",
    "                     refs = seq(0, 315, by = 45)) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    if (sampleAngle) {\n",
    "                      x = x[x$angle == sample(refs, size = 1), ]}\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using \"reshapeSS\" function, we will prepare the offline vs. online data and predict the result in \"estFold\" for 3 neighbors and 3 angles. Now we are ready to calculate the error. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "offline = offline[ offline$mac != \"00:0f:a3:39:dd:cd\", ]\n",
    "\n",
    "keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "\n",
    "onlineCVSummary = reshapeSS(offline, keepVars = keepVars, \n",
    "                            sampleAngle = TRUE)\n",
    "\n",
    "onlineFold = subset(onlineCVSummary, \n",
    "                    posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "offlineFold = subset(offlineSummary,\n",
    "                     posXY %in% permuteLocs[ , -1])\n",
    "\n",
    "estFold = predXY(newSignals = onlineFold[ , 6:11], \n",
    "                 newAngles = onlineFold[ , 4], \n",
    "                 offlineFold, numAngles = 3, k = 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we predict the result with different number of (k) neighbors, and calculate the error size per scenario. This data will help us find the best possible K using a brute force approach and as we hope the cross validation reduces the chance of overfitting. \n",
    "\n",
    "## What to expect\n",
    "Before running the plot, we expect that the estimate of many sensors that are far away from each other will not lead to accurate location. That is why a small K is expected to have the minimum root mean square error.   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "calcError(estFold, actualFold)\n",
    "\n",
    "K = 20\n",
    "err = rep(0, K)\n",
    "\n",
    "for (j in 1:v) {\n",
    "  onlineFold = subset(onlineCVSummary, \n",
    "                      posXY %in% permuteLocs[ , j])\n",
    "  offlineFold = subset(offlineSummary,\n",
    "                       posXY %in% permuteLocs[ , -j])\n",
    "  actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "  \n",
    "  for (k in 1:K) {\n",
    "    estFold = predXY(newSignals = onlineFold[ , 6:11],\n",
    "                     newAngles = onlineFold[ , 4], \n",
    "                     offlineFold, numAngles = 3, k = k)\n",
    "    err[k] = err[k] + calcError(estFold, actualFold)\n",
    "  }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will plot the error data per predicted location for each fold. We observe that root mean square error is minimized for k = 10 which stand somewhere between 1 and 20. As expected, the error rate will increase with too many neighbors (K > 10) contributing to the calculation of the location. We also observe that the calculation of one or two sensors leads to inaccurate results, but as closest neighbors contribute more to the calculation, the rate of error drastically decrease. \n",
    "\n",
    "## What we learn from this observation\n",
    "This observation proves that although KNN does not scale very well due to its complexity O(N^2), it yeilds tangible results for problems with relatively small input.   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pdf(file = \"Geo_CVChoiceOfK.pdf\", width = 10, height = 6)\n",
    "#oldPar = par(mar = c(4, 3, 1, 1))\n",
    "plot(y = err, x = (1:K),  type = \"l\", lwd= 2,\n",
    "     ylim = c(100, 2100),\n",
    "     xlab = \"Number of Neighbors\",\n",
    "     ylab = \"Sum of Square Errors\")\n",
    "\n",
    "rmseMin = min(err)\n",
    "kMin = which(err == rmseMin)[1]\n",
    "segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), \n",
    "         lty = 2, lwd = 2)\n",
    "segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, \n",
    "         col = grey(0.4), lty = 2, lwd = 2)\n",
    "text(x = kMin - 2, y = rmseMin + 40, \n",
    "     label = as.character(round(rmseMin)), col = grey(0.4))\n",
    "#par(oldPar)\n",
    "#dev.off()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#mtext(kMin, side = 1, line = 1, at = kMin, col = grey(0.4))\n",
    "\n",
    "\n",
    "estXYk5 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 5)\n",
    "\n",
    "calcError(estXYk5, actualXY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predXY = function(newSignals, newAngles, trainData, \n",
    "                  numAngles = 1, k = 3){\n",
    "  \n",
    "  closeXY = list(length = nrow(newSignals))\n",
    "  \n",
    "  for (i in 1:nrow(newSignals)) {\n",
    "    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)\n",
    "    closeXY[[i]] = findNN(newSignal = as.numeric(newSignals[i, ]),\n",
    "                           trainSS)\n",
    "  }\n",
    "\n",
    "  estXY = lapply(closeXY, function(x)\n",
    "                            sapply(x[ , 2:3], \n",
    "                                    function(x) mean(x[1:k])))\n",
    "  estXY = do.call(\"rbind\", estXY)\n",
    "  return(estXY)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question\n",
    "Conduct a more thorough data analysis into these two MAC addresses including determining locations by using data corresponding to both MAC addresses(with MAC address \"00:0f:a3:39:e1:c0\" and \"00:0f:a3:39:dd:cd\").  Which of these two MAC addresses should be used and which should not be used for RTLS? \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Access Point data Analysis\n",
    "The following code prints the number of existing data for \"00:0f:a3:39:e1:c0\" and \"00:0f:a3:39:dd:cd\" access points. We observe that the size of the data is very close. Plotting the data of both access points indicates that \"00:0f:a3:39:dd:cd\" data has lower signal strength, and we know that the signal strength has an impact on the location prediction.    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comparedAccessPoints = c(\"00:0f:a3:39:e1:c0\", \"00:0f:a3:39:dd:cd\")\n",
    "originalOffline = readData(subMacs = comparedAccessPoints)\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "library(lattice)\n",
    "bwplot(signal ~ factor(angle) | mac, data = originalOffline, \n",
    "       subset = posX == 2 & posY == 12, \n",
    "       layout = c(2,3))\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "table(originalOffline$mac)\n",
    "subMacs = names(sort(table(originalOffline$mac), decreasing = TRUE))[1:7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analysis\n",
    "Nolan and Temple Lang did not use one of the seven access points that exists in the offline data set. The access point \"00:0f:a3:39:dd:cd\" was removed by them in the analysis as it was roughly in the same location as \"00:0f:a3:39:e1:c0\". \n",
    "We will rerun all the steps again with \"00:0f:a3:39:dd:cd\" and without \"00:0f:a3:39:e1:c0\". We will observe the results with the requested access points data. Then we will do KNN with both data in the model and compare the root mean square error between predicted and actual locations. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subMacs = c(\"00:0f:a3:39:e1:c0\", \"00:0f:a3:39:dd:cd\", \"00:14:bf:b1:97:8a\",\n",
    "                       \"00:14:bf:3b:c7:c6\", \"00:14:bf:b1:97:90\", \"00:14:bf:b1:97:8d\", \"00:14:bf:b1:97:81\")\n",
    "filteredAccessPoint = \"00:0f:a3:39:e1:c0\"\n",
    "usedAccessPoints = subMacs[subMacs != filteredAccessPoint] \n",
    "offlineRedo = readData(subMacs = usedAccessPoints)\n",
    "\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "library(lattice)\n",
    "bwplot(signal ~ factor(angle) | mac, data = offlineRedo, \n",
    "       subset = posX == 2 & posY == 12 \n",
    "       & mac != filteredAccessPoint\n",
    "       , \n",
    "       layout = c(2,3))\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "#-----------------------------------------\n",
    "# In the following code, we compare the distributions of signal strength for different angles and MAC addresses \n",
    "# at the central location of x = 23 and y = 4. Lack of normal distribution in the following plot illustrates that\n",
    "# conditioning on angle and MAC address is warranted. If the distributions were normal, we could conclude that \n",
    "# signal can be received from different angles but that does not seem to be the case.\n",
    "#-----------------------------------------\n",
    "summary(offlineRedo$signal)\n",
    "\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "densityplot( ~ signal | mac + factor(angle), data = offlineRedo,\n",
    "             subset = posX == 24 & posY == 4  \n",
    "             & mac != filteredAccessPoint\n",
    "            ,bw = 0.5, plot.points = FALSE)\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "#-----------------------------------------\n",
    "# If we want to examine the distribution of signal strength for all 166 locations, 8 angles, and 6 access points, \n",
    "# we need to create thousands of boxplots or density curves. We can, instead, examine summary statistics such as \n",
    "# the mean and SD or the median and IQR of signal strength for all location–orientation–access point combinations. \n",
    "# For each combination, we have roughly 100 observations. To compute summary statistics for these various \n",
    "# combinations, we first create a special factor that contains all of the unique combinations of the observed (x, y)\n",
    "# pairs for the 166 locations.\n",
    "#-----------------------------------------\n",
    "# offline = offline[ offline$mac != \"00:0f:a3:39:dd:cd\", ]\n",
    "offlineRedo$posXY = paste(offlineRedo$posX, offlineRedo$posY, sep = \"-\")\n",
    "\n",
    "byLocAngleAP = with(offlineRedo, \n",
    "                    by(offlineRedo, list(posXY, angle, mac), \n",
    "                       function(x) x))\n",
    "\n",
    "signalSummary = \n",
    "  lapply(byLocAngleAP,            \n",
    "         function(oneLoc) {\n",
    "           ans = oneLoc[1, ]\n",
    "           ans$medSignal = median(oneLoc$signal)\n",
    "           ans$avgSignal = mean(oneLoc$signal)\n",
    "           ans$num = length(oneLoc$signal)\n",
    "           ans$sdSignal = sd(oneLoc$signal)\n",
    "           ans$iqrSignal = IQR(oneLoc$signal)\n",
    "           ans\n",
    "           })\n",
    "\n",
    "offlineSummary = do.call(\"rbind\", signalSummary)  \n",
    "                       \n",
    "#-----------------------------------------\n",
    "# We plot the offlineSummary data and observe that the weakest signals have small standard deviations and that \n",
    "# it appears that the SD increases with the average signal strength. If we plan to model the behavior of signal \n",
    "# strength, then we want to take these features into consideration. The weak signals have low variability and \n",
    "# the stronger signals have greater variability. \n",
    "#-----------------------------------------                       \n",
    "\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "breaks = seq(-90, -30, by = 5)\n",
    "bwplot(sdSignal ~ cut(avgSignal, breaks = breaks),\n",
    "       data = offlineSummary, \n",
    "       subset = mac != filteredAccessPoint,\n",
    "       xlab = \"Mean Signal\", ylab = \"SD Signal\")\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()   \n",
    "                       \n",
    "                       \n",
    "#-----------------------------------------\n",
    "# We examine the skewness of signal strength by plotting the difference, avgSignal - medSignal, against the number\n",
    "# of observations. Then we use the fitted model to predict the difference for each value of num and add \n",
    "# these predictions to the scatter plot. We see that these two measures of centrality are similar to each other; \n",
    "# they typically differ by less than 1 to 2 dBm.                       \n",
    "#-----------------------------------------                       \n",
    "                                           \n",
    "oldPar = par(mar = c(4.1, 4.1, 1, 1))\n",
    "\n",
    "with(offlineSummary,\n",
    "     smoothScatter((avgSignal - medSignal) ~ num,\n",
    "                   xlab = \"Number of Observations\", \n",
    "                   ylab = \"mean - median\"))\n",
    "abline(h = 0, col = \"#984ea3\", lwd = 2)\n",
    "\n",
    "lo.obj = \n",
    "  with(offlineSummary,\n",
    "       loess(diff ~ num, data = data.frame(diff = (avgSignal - medSignal), num = num)))\n",
    "\n",
    "lo.obj.pr = predict(lo.obj, newdata = data.frame(num = (70:120)))\n",
    "lines(x = 70:120, y = lo.obj.pr, col = \"#4daf4a\", lwd = 2)\n",
    "\n",
    "par(oldPar)\n",
    "dev.off()\n",
    " \n",
    "oneAPAngle = subset(offlineSummary, mac == subMacs[5] & angle == 0)         \n",
    "#-----------------------------------------\n",
    "#-----------------------------------------                       \n",
    "                       \n",
    "list.of.packages <- c(\"fields\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages)\n",
    "\n",
    "\n",
    "library(fields)\n",
    "smoothSS = Tps(oneAPAngle[, c(\"posX\",\"posY\")], oneAPAngle$avgSignal)\n",
    "\n",
    "vizSmooth = predictSurface(smoothSS)\n",
    "\n",
    "plot.surface(vizSmooth, type = \"C\")\n",
    "\n",
    "points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5)\n",
    "                       \n",
    "\n",
    "#-----------------------------------------\n",
    "# we see that we can easily identify the location of the access point as the dark red region at the top of \n",
    "# the “mountain.” We also confirm the effect of the orientation on signal strength. Additionally, a corridor \n",
    "# effect emerges. The signal is stronger relative to distance along the corridors where the signals are not \n",
    "# blocked by walls. We know the locations of the access points based on the floor plan of the building, but we have\n",
    "# not been given their exact location and we do not know the mapping between MAC address and access point. \n",
    "# Fortunately, the contour maps that we just created make it easy to connect the MAC address to the access point \n",
    "# marked on the floor plan in figure 1.1. For example, the signals appearing in the top row of the plot \n",
    "# clearly correspond to the access point in the top left corner of the building. Also, according to the \n",
    "# documentation, the training data were measured at 1 meter intervals in the building so we can use the grey dots\n",
    "# on the plan to estimate the location of the access points. We find that two MAC addresses have similar heat \n",
    "# maps and these both correspond to the access point near the center of the building (i.e., x = 7.5 and y = 6.3).\n",
    "#-----------------------------------------                                                                      \n",
    "#-----------------------------------------                         \n",
    "\n",
    "unique(offlineSummary$mac)\n",
    "                       \n",
    "surfaceSS = function(data, mac, angle = 45) {\n",
    "  require(fields)\n",
    "  oneAPAngle = data[ data$mac == mac & data$angle == angle, ]\n",
    "  smoothSS = Tps(oneAPAngle[, c(\"posX\",\"posY\")], oneAPAngle$avgSignal)\n",
    "  vizSmooth = predictSurface(smoothSS)\n",
    "  plot.surface(vizSmooth, type = \"C\", xlab = \"\", ylab = \"\", xaxt = \"n\", yaxt = \"n\")\n",
    "  points(oneAPAngle$posX, oneAPAngle$posY, pch=19, cex = 0.5) \n",
    "}\n",
    "\n",
    "parCur = par(mfrow = c(2,2), mar = rep(1, 4))\n",
    "\n",
    "\n",
    "mapply(surfaceSS, mac = usedAccessPoints[ rep(c(5, 2), each = 2) ], \n",
    "        angle = rep(c(0, 135), 2),\n",
    "        data = list(data = offlineSummary))\n",
    " \n",
    "  par(parCur)\n",
    "\n",
    " offlineSummary = subset(offlineSummary, mac != filteredAccessPoint)\n",
    "                       \n",
    "AP = matrix( c( 7.5, 6.3, 2.5, -.8, 12.8, -2.8,  \n",
    "                1, 14, 33.5, 9.3,  33.5, 2.8),\n",
    "            ncol = 2, byrow = TRUE,\n",
    "            dimnames = list(usedAccessPoints, c(\"x\", \"y\") ))\n",
    "   \n",
    "   \n",
    "\n",
    "#-----------------------------------------\n",
    "#-----------------------------------------                        \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "diffs = offlineSummary[ , c(\"posX\", \"posY\")] - AP[ offlineSummary$mac, ]\n",
    "\n",
    "offlineSummary$dist = sqrt(diffs[ , 1]^2 + diffs[ , 2]^2)\n",
    "\n",
    "xyplot(signal ~ dist | factor(mac) + factor(angle), \n",
    "       data = offlineSummary, pch = 19, cex = 0.3,\n",
    "       xlab =\"distance\")\n",
    "\n",
    "oldPar = par(mar = c(3.1, 3.1, 1, 1))\n",
    "library(lattice)\n",
    "xyplot(signal ~ dist | factor(mac) + factor(angle), \n",
    "       data = offlineSummary, pch = 19, cex = 0.3,\n",
    "       xlab =\"distance\")\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "macs = unique(offlineSummary$mac)                                                               "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we have a set of training data that we can use to predict the location of our new point. We want to look at the distance in terms of signal strengths from these training data to the new point. If we would like to use the nearest neighbor or the 3 nearest neighbors, we need to calculate the distance from the new point to all observations in the training set. The following code prepares the data the run the K nearest neighbors:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "online = readData(\"data/online.final.trace.txt\", subMacs = macs)\n",
    "\n",
    "online$posXY = paste(online$posX, online$posY, sep = \"-\")\n",
    "\n",
    "length(unique(online$posXY))\n",
    "\n",
    "tabonlineXYA = table(online$posXY, online$angle)\n",
    "tabonlineXYA[1:6, ]\n",
    "\n",
    "keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "byLoc = with(online, \n",
    "             by(online, list(posXY), \n",
    "                function(x) {\n",
    "                  ans = x[1, keepVars]\n",
    "                  avgSS = tapply(x$signal, x$mac, mean)\n",
    "                  y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                        dimnames = list(ans$posXY, names(avgSS)))\n",
    "                  cbind(ans, y)\n",
    "                }))\n",
    "\n",
    "onlineSummary = do.call(\"rbind\", byLoc)  \n",
    "dim(onlineSummary)\n",
    "\n",
    "names(onlineSummary)\n",
    "m = 3; angleNewObs = 230\n",
    "refs = seq(0, by = 45, length  = 8)\n",
    "nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "if (m %% 2 == 1) {\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "} else {\n",
    "  m = m + 1\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  if (sign(angleNewObs - nearestAngle) > -1) \n",
    "    angles = angles[ -1 ]\n",
    "  else \n",
    "    angles = angles[ -m ]\n",
    "}\n",
    "angles = angles + nearestAngle\n",
    "angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "angles[angles > 360] = angles[ angles > 360 ] - 360\n",
    "offlineSubset = \n",
    "  offlineSummary[ offlineSummary$angle %in% angles, ]\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\")) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}\n",
    "\n",
    "trainSS = reshapeSS(offlineSubset, varSignal = \"avgSignal\")\n",
    "\n",
    "selectTrain = function(angleNewObs, signals = NULL, m = 1) {\n",
    "  # m is the number of angles to keep between 1 and 5\n",
    "  refs = seq(0, by = 45, length  = 8)\n",
    "  nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "  if (m %% 2 == 1) \n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  else {\n",
    "    m = m + 1\n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "    if (sign(angleNewObs - nearestAngle) > -1) \n",
    "      angles = angles[ -1 ]\n",
    "    else \n",
    "      angles = angles[ -m ]\n",
    "  }\n",
    "  angles = angles + nearestAngle\n",
    "  angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "  angles[angles > 360] = angles[ angles > 360 ] - 360\n",
    "  angles = sort(angles) \n",
    "  \n",
    "  offlineSubset = signals[ signals$angle %in% angles, ]\n",
    "  reshapeSS(offlineSubset, varSignal = \"avgSignal\")\n",
    "}\n",
    "\n",
    "train130 = selectTrain(130, offlineSummary, m = 3)\n",
    "\n",
    "#head(train130)\n",
    "\n",
    "#length(train130[[1]])\n",
    "\n",
    "findNN = function(newSignal, trainSubset) {\n",
    "  diffs = apply(trainSubset[ , 4:9], 1, \n",
    "                function(x) x - newSignal)\n",
    "  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )\n",
    "  closest = order(dists)\n",
    "  return(trainSubset[closest, 1:3 ])\n",
    "}\n",
    "                \n",
    "predXY = function(newSignals, newAngles, trainData, numAngles = 1, k = 3) {\n",
    "  \n",
    "  closeXY = list(length = nrow(newSignals))\n",
    "  \n",
    "  for (i in 1:nrow(newSignals)) {\n",
    "    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)\n",
    "    closeXY[[i]] = \n",
    "      findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)\n",
    "  }\n",
    "\n",
    "  estXY = lapply(closeXY, \n",
    "                 function(x) sapply(x[ , 2:3], \n",
    "                                    function(x) mean(x[1:k])))\n",
    "  estXY = do.call(\"rbind\", estXY)\n",
    "  return(estXY)\n",
    "}\n",
    "estXYk3 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 3)\n",
    "                                    \n",
    "\n",
    "estXYk1 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 1)   \n",
    "                                    \n",
    "floorErrorMap = function(estXY, actualXY, trainPoints = NULL, AP = NULL) {\n",
    "  \n",
    "    plot(0, 0, xlim = c(0, 35), ylim = c(-3, 15), type = \"n\",\n",
    "         xlab = \"\", ylab = \"\", axes = FALSE)\n",
    "    cap <- \"Figure x: Prediction vs. Real Location\"\n",
    "    box()\n",
    "    if ( !is.null(AP) ) points(AP, pch = 15)\n",
    "    if ( !is.null(trainPoints) )\n",
    "      points(trainPoints, pch = 19, col=\"grey\", cex = 0.6)\n",
    "    \n",
    "    points(x = actualXY[, 1], y = actualXY[, 2], \n",
    "           pch = 19, cex = 0.8 )\n",
    "    points(x = estXY[, 1], y = estXY[, 2], \n",
    "           pch = 8, cex = 0.8 )\n",
    "    segments(x0 = estXY[, 1], y0 = estXY[, 2],\n",
    "             x1 = actualXY[, 1], y1 = actualXY[ , 2],\n",
    "             lwd = 2, col = \"red\")\n",
    "}\n",
    "\n",
    "trainPoints = offlineSummary[ offlineSummary$angle == 0 & \n",
    "                              offlineSummary$mac == \"00:0f:a3:39:dd:cd\" ,\n",
    "                        c(\"posX\", \"posY\")]                                    \n",
    "\n",
    "pdf(file=\"GEO_FloorPlanK3Errors.pdf\", width = 10, height = 7)\n",
    "oldPar = par(mar = c(1, 1, 1, 1))\n",
    "floorErrorMap(estXYk3, onlineSummary[ , c(\"posX\",\"posY\")], \n",
    "              trainPoints = trainPoints, AP = AP)\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "#pdf(file=\"GEO_FloorPlanK1Errors.pdf\", width = 10, height = 7)\n",
    "oldPar = par(mar = c(1, 1, 1, 1))\n",
    "floorErrorMap(estXYk1, onlineSummary[ , c(\"posX\",\"posY\")], \n",
    "              trainPoints = trainPoints, AP = AP)\n",
    "par(oldPar)\n",
    "dev.off()\n",
    "\n",
    "calcError = \n",
    "function(estXY, actualXY) \n",
    "   sum( rowSums( (estXY - actualXY)^2) )\n",
    "\n",
    "actualXY = onlineSummary[ , c(\"posX\", \"posY\")]\n",
    "sapply(list(estXYk1, estXYk3), calcError, actualXY)\n",
    "\n",
    "                                    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting the location with new access point data \n",
    "The following code prepares the essential folds of offline data (with the new access point data) so that we can predict the location and calculate the root mean square error rate for different number of neighbors in the range of 1 to 20. This will help us plot the root mean square error vs. the number of neighbors and analyze the way new access point contributes to our prediction model. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v = 11\n",
    "permuteLocs = sample(unique(offlineSummary$posXY))\n",
    "permuteLocs = matrix(permuteLocs, ncol = v, \n",
    "                     nrow = floor(length(permuteLocs)/v))\n",
    "\n",
    "onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\"),\n",
    "                     sampleAngle = FALSE, \n",
    "                     refs = seq(0, 315, by = 45)) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    if (sampleAngle) {\n",
    "                      x = x[x$angle == sample(refs, size = 1), ]}\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 6,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}\n",
    "\n",
    "\n",
    "offlineRedo = offlineRedo[ offlineRedo$mac != filteredAccessPoint, ]\n",
    "\n",
    "keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "\n",
    "onlineCVSummary = reshapeSS(offlineRedo, keepVars = keepVars, \n",
    "                            sampleAngle = TRUE)\n",
    "\n",
    "onlineFold = subset(onlineCVSummary, \n",
    "                    posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "offlineFold = subset(offlineSummary,\n",
    "                     posXY %in% permuteLocs[ , -1])\n",
    "\n",
    "estFold = predXY(newSignals = onlineFold[ , 6:11], \n",
    "                 newAngles = onlineFold[ , 4], \n",
    "                 offlineFold, numAngles = 3, k = 3)\n",
    "\n",
    "actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "calcError(estFold, actualFold)\n",
    "\n",
    "K = 20\n",
    "err = rep(0, K)\n",
    "\n",
    "for (j in 1:v) {\n",
    "  onlineFold = subset(onlineCVSummary, \n",
    "                      posXY %in% permuteLocs[ , j])\n",
    "  offlineFold = subset(offlineSummary,\n",
    "                       posXY %in% permuteLocs[ , -j])\n",
    "  actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "  \n",
    "  for (k in 1:K) {\n",
    "    estFold = predXY(newSignals = onlineFold[ , 6:11],\n",
    "                     newAngles = onlineFold[ , 4], \n",
    "                     offlineFold, numAngles = 3, k = k)\n",
    "    err[k] = err[k] + calcError(estFold, actualFold)\n",
    "  }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the root mean square error rate\n",
    "When we compare this plot with Nolan and Lang code, we observe that our access point (\"00:0f:a3:39:dd:cd\") leads to a smaller root mean square error rate with the same number of neighbors(k=5) and the \"calcError\" function yields almost the same number (249.9243). \n",
    "\n",
    "# Conclusion\n",
    "We conclude that the access point \"00:0f:a3:39:dd:cd\" has a better contribution to the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(y = err, x = (1:K),  type = \"l\", lwd= 2,\n",
    "     ylim = c(100, 2100),\n",
    "     xlab = \"Number of Neighbors\",\n",
    "     ylab = \"Sum of Square Errors\")\n",
    "\n",
    "rmseMin = min(err)\n",
    "rmseMin\n",
    "kMin = which(err == rmseMin)[1]\n",
    "segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), \n",
    "         lty = 2, lwd = 2)\n",
    "segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, \n",
    "         col = grey(0.4), lty = 2, lwd = 2)\n",
    "text(x = kMin - 2, y = rmseMin + 40, \n",
    "     label = as.character(round(rmseMin)), col = grey(0.4))\n",
    "\n",
    "estXYk5 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 5)\n",
    "\n",
    "calcError(estXYk5, actualXY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Having both access points\n",
    "Now we would like to know whether we can have a better model with both access points in the data. As they are roughly in the same location, the result might not differ as much. But we try to find an answer for this question in the following: "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subMacs = c(\"00:0f:a3:39:e1:c0\", \"00:0f:a3:39:dd:cd\", \"00:14:bf:b1:97:8a\",\n",
    "                       \"00:14:bf:3b:c7:c6\", \"00:14:bf:b1:97:90\", \"00:14:bf:b1:97:8d\",\n",
    "                       \"00:14:bf:b1:97:81\")\n",
    "filteredAccessPoint = \"\"\n",
    "usedAccessPoints = subMacs \n",
    "offlineRedo = readData()\n",
    "\n",
    "oldPar = par(mar = c(3.1, 3, 1, 1))\n",
    "\n",
    "offlineRedo$posXY = paste(offlineRedo$posX, offlineRedo$posY, sep = \"-\")\n",
    "\n",
    "byLocAngleAP = with(offlineRedo, \n",
    "                    by(offlineRedo, list(posXY, angle, mac), \n",
    "                       function(x) x))\n",
    "\n",
    "signalSummary = \n",
    "  lapply(byLocAngleAP,            \n",
    "         function(oneLoc) {\n",
    "           ans = oneLoc[1, ]\n",
    "           ans$medSignal = median(oneLoc$signal)\n",
    "           ans$avgSignal = mean(oneLoc$signal)\n",
    "           ans$num = length(oneLoc$signal)\n",
    "           ans$sdSignal = sd(oneLoc$signal)\n",
    "           ans$iqrSignal = IQR(oneLoc$signal)\n",
    "           ans\n",
    "           })\n",
    "\n",
    "offlineSummary = do.call(\"rbind\", signalSummary)                         \n",
    "\n",
    "macs = unique(offlineSummary$mac)    \n",
    "macs                       \n",
    "    \n",
    "online = readData(\"data/online.final.trace.txt\", subMacs = macs)\n",
    "\n",
    "online$posXY = paste(online$posX, online$posY, sep = \"-\")\n",
    "\n",
    "length(unique(online$posXY))\n",
    "\n",
    "tabonlineXYA = table(online$posXY, online$angle)\n",
    "tabonlineXYA[1:7, ]\n",
    "\n",
    "keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "byLoc = with(online, \n",
    "             by(online, list(posXY), \n",
    "                function(x) {\n",
    "                  ans = x[1, keepVars]\n",
    "                  avgSS = tapply(x$signal, x$mac, mean)\n",
    "                  y = matrix(avgSS, nrow = 1, ncol = 7,\n",
    "                        dimnames = list(ans$posXY, names(avgSS)))\n",
    "                  cbind(ans, y)\n",
    "                }))\n",
    "\n",
    "onlineSummary = do.call(\"rbind\", byLoc)  \n",
    "\n",
    "\n",
    "names(onlineSummary)\n",
    "m = 3; angleNewObs = 230\n",
    "refs = seq(0, by = 45, length  = 8)\n",
    "nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "if (m %% 2 == 1) {\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "} else {\n",
    "  m = m + 1\n",
    "  angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  if (sign(angleNewObs - nearestAngle) > -1) \n",
    "    angles = angles[ -1 ]\n",
    "  else \n",
    "    angles = angles[ -m ]\n",
    "}\n",
    "angles = angles + nearestAngle\n",
    "angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "angles[angles > 360] = angles[ angles > 360 ] - 360\n",
    "offlineSubset = \n",
    "  offlineSummary[ offlineSummary$angle %in% angles, ]\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\")) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 7,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}\n",
    "\n",
    "trainSS = reshapeSS(offlineSubset, varSignal = \"avgSignal\")\n",
    "\n",
    "selectTrain = function(angleNewObs, signals = NULL, m = 1) {\n",
    "  # m is the number of angles to keep between 1 and 5\n",
    "  refs = seq(0, by = 45, length  = 8)\n",
    "  nearestAngle = roundOrientation(angleNewObs)\n",
    "  \n",
    "  if (m %% 2 == 1) \n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "  else {\n",
    "    m = m + 1\n",
    "    angles = seq(-45 * (m - 1) /2, 45 * (m - 1) /2, length = m)\n",
    "    if (sign(angleNewObs - nearestAngle) > -1) \n",
    "      angles = angles[ -1 ]\n",
    "    else \n",
    "      angles = angles[ -m ]\n",
    "  }\n",
    "  angles = angles + nearestAngle\n",
    "  angles[angles < 0] = angles[ angles < 0 ] + 360\n",
    "  angles[angles > 360] = angles[ angles > 360 ] - 360\n",
    "  angles = sort(angles) \n",
    "  \n",
    "  offlineSubset = signals[ signals$angle %in% angles, ]\n",
    "  reshapeSS(offlineSubset, varSignal = \"avgSignal\")\n",
    "}\n",
    "\n",
    "train130 = selectTrain(130, offlineSummary, m = 3)\n",
    "\n",
    "findNN = function(newSignal, trainSubset) {\n",
    "  diffs = apply(trainSubset[ , 4:9], 1, \n",
    "                function(x) x - newSignal)\n",
    "  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )\n",
    "  closest = order(dists)\n",
    "  return(trainSubset[closest, 1:3 ])\n",
    "}\n",
    "                \n",
    "predXY = function(newSignals, newAngles, trainData, numAngles = 1, k = 3) {\n",
    "  \n",
    "  closeXY = list(length = nrow(newSignals))\n",
    "  \n",
    "  for (i in 1:nrow(newSignals)) {\n",
    "    trainSS = selectTrain(newAngles[i], trainData, m = numAngles)\n",
    "    closeXY[[i]] = \n",
    "      findNN(newSignal = as.numeric(newSignals[i, ]), trainSS)\n",
    "  }\n",
    "\n",
    "  estXY = lapply(closeXY, \n",
    "                 function(x) sapply(x[ , 2:3], \n",
    "                                    function(x) mean(x[1:k])))\n",
    "  estXY = do.call(\"rbind\", estXY)\n",
    "  return(estXY)\n",
    "}\n",
    "                                    \n",
    "estXYk3 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 3)\n",
    "                                    \n",
    "\n",
    "estXYk1 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 1)   \n",
    "                                    \n",
    "\n",
    "calcError = \n",
    "function(estXY, actualXY) \n",
    "   sum( rowSums( (estXY - actualXY)^2) )\n",
    "\n",
    "actualXY = onlineSummary[ , c(\"posX\", \"posY\")]\n",
    "sapply(list(estXYk1, estXYk3), calcError, actualXY)                       \n",
    "\n",
    "                                    \n",
    "v = 11\n",
    "permuteLocs = sample(unique(offlineSummary$posXY))\n",
    "permuteLocs = matrix(permuteLocs, ncol = v, \n",
    "                     nrow = floor(length(permuteLocs)/v))\n",
    "\n",
    "onlineFold = subset(offlineSummary, posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "reshapeSS = function(data, varSignal = \"signal\", \n",
    "                     keepVars = c(\"posXY\", \"posX\",\"posY\"),\n",
    "                     sampleAngle = FALSE, \n",
    "                     refs = seq(0, 315, by = 45)) {\n",
    "  byLocation =\n",
    "    with(data, by(data, list(posXY), \n",
    "                  function(x) {\n",
    "                    if (sampleAngle) {\n",
    "                      x = x[x$angle == sample(refs, size = 1), ]}\n",
    "                    ans = x[1, keepVars]\n",
    "                    avgSS = tapply(x[ , varSignal ], x$mac, mean)\n",
    "                    y = matrix(avgSS, nrow = 1, ncol = 7,\n",
    "                               dimnames = list(ans$posXY,\n",
    "                                               names(avgSS)))\n",
    "                    cbind(ans, y)\n",
    "                  }))\n",
    "\n",
    "  newDataSS = do.call(\"rbind\", byLocation)\n",
    "  return(newDataSS)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    " keepVars = c(\"posXY\", \"posX\",\"posY\", \"orientation\", \"angle\")\n",
    "\n",
    " onlineCVSummary = reshapeSS(offlineRedo, keepVars = keepVars, \n",
    "                             sampleAngle = TRUE)\n",
    "\n",
    " onlineFold = subset(onlineCVSummary, \n",
    "                     posXY %in% permuteLocs[ , 1])\n",
    "\n",
    "offlineFold = subset(offlineSummary,\n",
    "                     posXY %in% permuteLocs[ , -1])\n",
    "\n",
    "estFold = predXY(newSignals = onlineFold[ , 6:11], \n",
    "                 newAngles = onlineFold[ , 4], \n",
    "                 offlineFold, numAngles = 3, k = 3)\n",
    "\n",
    "actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "calcError(estFold, actualFold)\n",
    "\n",
    "K = 20\n",
    "err = rep(0, K)\n",
    "\n",
    "for (j in 1:v) {\n",
    "  onlineFold = subset(onlineCVSummary, \n",
    "                      posXY %in% permuteLocs[ , j])\n",
    "  offlineFold = subset(offlineSummary,\n",
    "                       posXY %in% permuteLocs[ , -j])\n",
    "  actualFold = onlineFold[ , c(\"posX\", \"posY\")]\n",
    "  \n",
    "  for (k in 1:K) {\n",
    "    estFold = predXY(newSignals = onlineFold[ , 6:11],\n",
    "                     newAngles = onlineFold[ , 4], \n",
    "                     offlineFold, numAngles = 3, k = k)\n",
    "    err[k] = err[k] + calcError(estFold, actualFold)\n",
    "  }\n",
    "}                                                      "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the root mean square error rate with both access points\n",
    "The following plot illustrates a similiar trend. Very few access points and too many of them \n",
    "contribute to the root mean square error of predicted location. When we compare this plot with the corresponding previous \n",
    "plots , we observe that having both access points as part of the model will lead to a lower root mean square error (1337) compared to the original approach but it is not as good as having only \"00:0f:a3:39:dd:cd\" access point. The result makes sense as \"00:0f:a3:39:e1:c0\", \"00:0f:a3:39:dd:cd\" access points are close to each other and their combination should not significantly contribute to the accuracy of the predicted locations (if we disregard the impact of signal strength)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot(y = err, x = (1:K),  type = \"l\", lwd= 2,\n",
    "     ylim = c(100, 2100),\n",
    "     xlab = \"Number of Neighbors\",\n",
    "     ylab = \"Sum of Square Errors\")\n",
    "\n",
    "rmseMin = min(err)\n",
    "rmseMin\n",
    "kMin = which(err == rmseMin)[1]\n",
    "segments(x0 = 0, x1 = kMin, y0 = rmseMin, col = gray(0.4), \n",
    "         lty = 2, lwd = 2)\n",
    "segments(x0 = kMin, x1 = kMin, y0 = 1100,  y1 = rmseMin, \n",
    "         col = grey(0.4), lty = 2, lwd = 2)\n",
    "text(x = kMin - 2, y = rmseMin + 40, \n",
    "     label = as.character(round(rmseMin)), col = grey(0.4))\n",
    "\n",
    "estXYk5 = predXY(newSignals = onlineSummary[ , 6:11], \n",
    "                 newAngles = onlineSummary[ , 4], \n",
    "                 offlineSummary, numAngles = 3, k = 5)\n",
    "\n",
    "calcError(estXYk5, actualXY)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question\n",
    "While k-nearest neighbors has proven to be a good approach to determining location, alternate approaches have been proposed.  One simple alternative approach is to use weights on the received signal strength, where the weight is inversely proportional to the “distance” from the test observation.  This allows for the “nearest” points to have a greater contribution to the k-nearest neighbor location calculation than the points that are “further” away.  \n",
    "\n",
    "Implement this alternative prediction method.  For what range of values of weights are you able to obtain better prediction values than for the unweighted k-nearest neighbor approach? Use calcError() to compare this approach to the simple average.\n",
    "\n",
    "## Solution\n",
    "To measure the distance between points A and B in a feature space, various distance functions have been used in the literature, in which the Euclidean distance function is the most widely used one. Let A and B are represented by feature vectors A = (x1, x2, …, xm) and B = (y1, y2, …, ym),  where m is the dimensionality of the feature space. To calculate the distance between A and B, the normalized Euclidean metric is generally used by\n",
    "\n",
    "dist(A,B)=$\\sqrt{\\sum_{i=1}^{i=m}(X_i^-Y_i)^2/m}$\n",
    "\n",
    "The weighted KNN will only add an additional weight factor into the above equation.\n",
    "I tried to decouple the distance function from findNN although the algorithm is partially implemented in predXY function. Following is what I did: \n",
    " findNN function should be implemented in two ways to address both average and weighted algorithm. Following code implements findNN using this approach and the rest of the code can be called around this functionality. However, we need to consider that the current findNN returns the average of the 3 closest results. The following implementation provides the opportunity to pass the number of access points involved in the calculation. The hardest part of the following solution is to predict the result based on the findNN with weightedKnn function. It returns additional column that need to be considered in the way \"predXY\" function computes the KNN in estXY variable. I tried so hard to calculate that but I could not make it as it takes a long time to try different approaches due to the size of the code. But if the weight of each access point is multiplied to the value(the weight of the closest one will become one), and then the average of the values is computed, the closest access points will have a higher impact on the predicted results.   "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "findNN = function(newSignal, trainSubset, numberOfAccessPoints, algorithmFunction) {\n",
    "  diffs = apply(trainSubset[ , 4:9], 1, function(x) x - newSignal)  \n",
    "  return (apply(diffs, trainSubset, numberOfAccessPoints, algorithmFunction)) \n",
    "}\n",
    "\n",
    "averageKnnFunction = function(dists, trainSubset, numberOfAccessPoints) {\n",
    "  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )\n",
    "  closest = order(dists) \n",
    "  return (trainSubset[closest, 1:numberOfAccessPoints ]) \n",
    "}                   \n",
    "                \n",
    "weightedKnnFunction = function(diffs, trainSubset, numberOfAccessPoints) {\n",
    "  dists = apply(diffs, 2, function(x) sqrt(sum(x^2)) )\n",
    "  closest = order(dists)\n",
    "  closeXY = trainSubset[closest, 1:numberOfAccessPoints ]\n",
    "  weight = as.numeric(1/dists[closest]) \n",
    "  return (cbind(closeXY, weight)) \n",
    "}                                       "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "KNN is a good algorithm that can be used for relatively small datasets due to its scalability issue. In this exercise, the impact of certain access point that Nolan and Lang removed from their original dataset was evaluated mostly based on the root mean square error. The final decisions were made assuming that the signal strengh has consistent impact on the KNN prediction. Considering other orientation rounding techniques and applying signal strength in other ways can be an effective way to improve the model.\n",
    "\n",
    "Folding the offline data causes a irreproducable result. I think there should be a way to harness the randomness of folding so that the reader can reproduce the same result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reference\n",
    "Nolan, D., Temple Lang, D. DATA SCIENCE IN R: a Case Studies Approach to Computational Reasoning and Problem Solving. CRC PRESS, 2017\n",
    "\n",
    "The distance function effect on k-nearest neighbor classification for medical datasets. [Link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}